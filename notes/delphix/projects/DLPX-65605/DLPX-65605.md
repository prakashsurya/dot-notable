---
title: delphix/projects/DLPX-65605
created: '1970-01-01T00:00:00.000Z'
modified: '2019-10-01T01:41:43.380Z'
---

# DLPX-65605 Timeout waiting for zvol link to be created

* https://jira.delphix.com/browse/DLPX-65605

## Notes

* PR: https://github.com/delphix/zfs/pull/79

* Scale systems:
  * New: scale-dlpx-trunk-L400MS-2.scale-dc.delphix.com
  * New: scale-dlpx-trunk-l200ms-2.scale-dc.delphix.com
  * Old: scale-dlpx-trunk-l100ms-2.scale-dc.delphix.com

## Next

1. Wait for the issue to reproduce with my ZFS and "delphix-stat"
   changes deployed on "scale-dlpx-trunk-l200ms-2"

## Follow Up

* Add more information to "delphix-stats" service:
  * off-cpu stack information (for flamegraphs)
  * on-cpu stack information (for flamegraphs)

* Develop BPF script for taskq information

## Log

### 2019.9.30

The issue triggered again with my fix; need to understand why.

I have some changes to the "delphix-stat" service to collect on-cpu and
off-cpu information, which should help me understand why the issue is
still occuring the next time it triggers.

The original scalability system is having some issues (refreshes failing
for unrelated reasons), so I'm now using "scale-dlpx-trunk-l200ms-2".
I've upgraded the system to contain my ZFS fix, and have loaded my
changes to the "delphix-stat" service. The next time the issue crops up
on this system, the additional information collected via "delphix-stat"
should help shed some light on the issue.

### 2019.9.27

The issue reproduced again. Thus, I created an upgrade image with my
proposed ZFS change, and am attempting to upgrade the system.

So far, the upgrade hasn't been successful, due to two unrelated bugs in
upgrade: DLPX-66361 and DLPX-66369. I've applied workarounds to the
system to address these, and am retrying the upgrade.

Upgrade worked with fixes in place; running ZFS with my changes:

    $ sudo journalctl -b | grep ZFS:
    Sep 27 20:13:33 localhost kernel: ZFS: Loaded module v0.8.0-delphix+2019.09.26.17-7992c80, ZFS pool version 5000, ZFS filesystem version 5

### 2019.9.24

I've opened a PR to remove the call to "taskq_wait_outstanding":

* https://github.com/delphix/zfs/pull/79

### 2019.9.23

When this occurs, we see many hundreds of entries on the "z_zvol" taskq
(based on inspection of the /proc/spl/taskq-all file). Many of the
entries on the taskq end up in the "zvol_remove_minors_impl" function,
and I've measured the latency of that function:

    Function = zvol_remove_minors_impl
    msecs               : count     distribution
        0 -> 1          : 0        |                                        |
        2 -> 3          : 0        |                                        |
        4 -> 7          : 1        |                                        |
        8 -> 15         : 0        |                                        |
       16 -> 31         : 0        |                                        |
       32 -> 63         : 0        |                                        |
       64 -> 127        : 1        |                                        |
      128 -> 255        : 45       |****************************************|
      256 -> 511        : 5        |****                                    |

That data is from a 10 second sample, using the BCC "funclatency" tool.
As we can see, in this 10 second sample, most calls took 128ms at a
minimum.

Thus, some basic math tells us that in any 20 second interval, we could
only process at most about 150 removals, which is much less than the
400+ that'll occur based on the workload.

When profiling the "zvol_remove_minors_impl" function, I saw that most
of the time in the function was spent off-cpu, blocked in the function
"taskq_wait_outstanding".

### 2019.9.19

It took awhile, but this issue did reproduce on the scalability system.
Now we have a systemt that'll repdocue the issue, we need to use it to
answer the following questions:

1. What is the current throughput?
   * Create new BPF script to measure

2. Why is the throughput what it is?
   * Create new flamegraphs for on-cpu and off-cpu

3. What do we need the throughput to be?
   * Determine max number of VDBs
   * Determine number of ZVOLs per VDB
   * Determine max VDB refresh rate
   * Determine the product of above values

### 2019.9.18

With help from Kevin, we've regenerated the DE (again) and bumped the
number of VDBs to 400, and are waiting to see if it'll reproduce with
more VDBs.

### 2019.9.17

I haven't been able to reproduce the error on the original DE where the
issue was previously occuring. With that said, this original DE has been
regenerated since we last saw the issue, so the version of the software
that's currently running on the system isn't exactly the same software
that was previously running (e.g. both zfs and the kernel versions have
changed).

With about 110 VDBs running on this system, set to a refresh interval of
15 minutes each, the issue has not cropped up yet.

### 2019.9.16

According to John as pointed out in the bug, the issue appears to be a
combination of the following factors:

1. The function used to remove ZVOLs can take a relatively long time to
   remove a single ZVOL.
2. Each ZVOL is removed in a single threaded manner.
3. Each ZVOL is created in a single threaded manner.
4. A relatively large bach of ZVOL removals (prior to a ZVOL create) can
   significantly impact ZVOL creation latency due to the 3 points above.
